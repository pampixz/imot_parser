import scrapy
import re
import logging
from imot_bg.items import ImotItem
from datetime import datetime
from scrapy_playwright.page import PageMethod
import asyncio

logger = logging.getLogger(__name__)

class ImotBgSpider(scrapy.Spider):
    name = 'imot_debug'
    allowed_domains = ['imot.bg', 'www.imot.bg']

    custom_settings = {
        'CONCURRENT_REQUESTS': 4,
        'DOWNLOAD_DELAY': 2,
        'RETRY_TIMES': 2,
        'AUTOTHROTTLE_ENABLED': True,
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 60000,
    }

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
        "Accept-Language": "bg-BG,bg;q=0.9,en;q=0.8",
        "Referer": "https://www.imot.bg/"
    }

    def __init__(self, city='–°–æ—Ñ–∏—è', district='', *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.city = city
        self.district = district.strip().lower()
        if not self.district:
            raise ValueError("‚ùå –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç 'district' –Ω–µ —É–∫–∞–∑–∞–Ω")
        logger.info(f"üõ†Ô∏è –ü–∞—É–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è –≥–æ—Ä–æ–¥–∞: {self.city}, —Ä–∞–π–æ–Ω: {self.district}")

    def start_requests(self):
        url = f'https://www.imot.bg/obiavi/prodazhbi/grad-sofiya/{self.district}'
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å URL: {url}")

        yield scrapy.Request(
            url=url,
            callback=self.parse_search_results,
            meta={
                'playwright': True,
                'playwright_include_page': True,
                'playwright_page_methods': [
                    PageMethod("wait_for_selector", "div.item", timeout=60000)
                ],
                'page': 1,
                'district': self.district
            },
            headers=self.headers,
            errback=self.parse_error
        )

    def parse_search_results(self, response):
        current_page = response.meta.get('page', 1)
        district = response.meta.get('district', 'unknown')

        logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {current_page} | URL: {response.url}")

        page = response.meta.get("playwright_page")
        if page:
            asyncio.create_task(page.close())

        if "captcha" in response.text.lower():
            logger.error("–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞! –ü—Ä–æ–ø—É—Å–∫–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return

        listings = response.css('div.item')
        if not listings:
            logger.warning(f"‚ö†Ô∏è –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {current_page} –Ω–µ—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
            return

        for item in listings:
            try:
                relative_url = item.css('a::attr(href)').get()
                full_url = response.urljoin(relative_url) if relative_url else None

                if not full_url:
                    logger.warning("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –±–µ–∑ URL")
                    continue

                # —Å—Ä–∞–∑—É –∏–¥—ë–º –≤ parse_listing
                yield response.follow(
                    full_url,
                    callback=self.parse_listing,
                    meta={'district': district, 'page': current_page},
                    headers=self.headers,
                    errback=self.parse_error
                )
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {str(e)}")

        # –ø–∞–≥–∏–Ω–∞—Ü–∏—è
        next_page = response.css('a.next::attr(href)').get()
        if next_page:
            yield response.follow(
                next_page,
                callback=self.parse_search_results,
                meta={
                    'playwright': True,
                    'playwright_include_page': True,
                    'playwright_page_methods': [
                        PageMethod("wait_for_selector", "div.item", timeout=60000)
                    ],
                    'page': current_page + 1,
                    'district': district
                },
                headers=self.headers,
                errback=self.parse_error
            )

    def parse_listing(self, response):
        logger.info(f"üè† –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {response.url}")

        item = ImotItem()
        description = self.extract_description(response)

        rooms_text = response.xpath("//div[contains(text(), '–¢–∏–ø –∏–º–æ—Ç')]/strong/text()").get() or ''
        item_rooms = self.determine_room_count(rooms_text)

        item.update({
            'source': 'imot.bg',
            'source_id': self.extract_id_from_url(response.url),
            'title': self.clean(response.css("div.advHeader div.title::text").get()),
            'currency': 'EUR',
            'price': self.clean_price(response.xpath("//div[@id='cena']/text()").get()),
            'price_sqm': self.clean_price(response.xpath("//span[@id='cenakv']/text()").get()),
            'area': self.clean_area(response.xpath("//div[contains(text(), '–ü–ª–æ—â')]/strong/text()").get()),
            'rooms': item_rooms,
            'floor': self.clean(response.xpath("//div[contains(text(), '–ï—Ç–∞–∂')]/strong/text()").get()),
            'construction_type': self.clean(
                response.xpath("//div[contains(text(), '–°—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ')]/strong[1]/text()").get()),
            'year_built': self.extract_year(
                response.xpath("//div[contains(text(), '–°—Ç—Ä–æ–∏—Ç–µ–ª—Å—Ç–≤–æ')]/strong[2]/text()").get()),
            'description': description,
            'location': self.clean(response.css("div.location::text").get()),
            'district': response.meta.get('district'),
            'city': self.city,
            'agency': self.clean(response.css("div.name::text").get()),
            'phone': self.clean(response.css("div.phone::text").get()),
            'page_found': response.meta.get('page', 1),
            'url': response.url,
        })

        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω item: {item.get('title')} ‚Äî {item.get('price')} EUR")
        yield item

    def parse_error(self, failure):
        logger.error(f"üî• –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {failure.value}")

    @staticmethod
    def extract_description(response):
        description_html = response.xpath("//div[@id='description_div']").get()
        if description_html:
            text = re.sub(r'<br\s*/?>', '\n', description_html)
            return re.sub(r'<[^>]+>', '', text).strip()
        return None

    @staticmethod
    def determine_room_count(text):
        room_map = {
            "–µ–¥–Ω–æ—Å—Ç–∞–µ–Ω": 1, "1-—Å—Ç–∞–µ–Ω": 1,
            "–¥–≤—É—Å—Ç–∞–µ–Ω": 2, "2-—Å—Ç–∞–µ–Ω": 2,
            "—Ç—Ä–∏—Å—Ç–∞–µ–Ω": 3, "3-—Å—Ç–∞–µ–Ω": 3,
            "–º–Ω–æ–≥–æ—Å—Ç–∞–µ–Ω": 4
        }
        return next((v for k, v in room_map.items() if k in text.lower()), None)

    @staticmethod
    def clean(text):
        return text.strip() if text else None

    @staticmethod
    def extract_id_from_url(url):
        match = re.search(r'obiava-([\w]+)', url)
        return match.group(1) if match else None

    @staticmethod
    def clean_price(text):
        if not text:
            return None
        try:
            num = re.sub(r'[^\d.]', '', text.replace(' ', '').replace(',', '.'))
            return float(num) if num else None
        except (ValueError, TypeError):
            return None

    @staticmethod
    def clean_area(text):
        if not text:
            return None
        try:
            num = re.search(r'([\d,.]+)', text)
            return float(num.group(1).replace(',', '.')) if num else None
        except (ValueError, TypeError):
            return None

    @staticmethod
    def extract_year(text):
        if not text:
            return None
        match = re.search(r'\d{4}', text)
        return int(match.group()) if match else None
